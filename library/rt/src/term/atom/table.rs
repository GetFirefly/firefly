use alloc::collections::btree_map::BTreeMap;
use core::alloc::Layout;
use core::fmt;
use core::mem;
use core::ptr::{self, NonNull};
use core::slice;
use core::str;

use firefly_arena::DroplessArena;
use firefly_system::sync::{OnceLock, RwLock, RwLockReadGuard, RwLockWriteGuard};

use super::{Atom, AtomError};

/// The atom table used by the runtime system
static ATOMS: OnceLock<RwLock<AtomTable>> = OnceLock::new();

#[derive(Copy, Clone, Debug)]
pub struct TryAtomFromTermError(pub &'static str);
impl fmt::Display for TryAtomFromTermError {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        write!(f, "atom `{}` is not supported", self.0)
    }
}

/// This struct matches the layout of raw atom metadata in memory.
///
/// The compiler generates a record of this type for each atom, referencing the string
/// value of the atom elsewhere in memory (where depends on whether the atom was generated at
/// compile-time or runtime).
///
/// NOTE: This struct must have a size that is a power of 8
#[derive(Debug, Copy, Clone)]
#[repr(C, align(8))]
pub struct AtomData {
    pub size: usize,
    pub ptr: *const u8,
}
unsafe impl Send for AtomData {}
unsafe impl Sync for AtomData {}
impl AtomData {
    #[inline]
    pub unsafe fn as_str(&self) -> Option<&'static str> {
        str::from_utf8(self.as_bytes()).ok()
    }

    #[inline(always)]
    pub unsafe fn as_bytes(&self) -> &'static [u8] {
        slice::from_raw_parts::<'static, _>(self.ptr, self.size)
    }
}

/// Performs one-time initialization of the atom table at program start, using the
/// array of constant atom values present in the compiled program.
///
/// It is expected that this will be called by code generated by the compiler, during the
/// earliest phase of startup, to ensure that nothing has tried to use the atom table yet.
#[export_name = "__firefly_initialize_atom_table"]
pub unsafe extern "C-unwind" fn init(start: *const AtomData, end: *const AtomData) -> bool {
    if start == end {
        return true;
    }
    if start.is_null() || end.is_null() {
        return false;
    }

    debug_assert_eq!(
        ((end as usize) - (start as usize)) % mem::size_of::<AtomData>(),
        0,
        "invalid atom data range"
    );
    let len = end.offset_from(start);
    let data = slice::from_raw_parts::<'static, _>(start, len as usize);

    with_atom_table(|mut atoms| {
        atoms.extend(data);
    });

    true
}

/// Like `get_data_or_insert`, but optimized for the case where the given atom value has static
/// lifetime, and thus doesn't require allocating space for and cloning the value. This is faster in
/// that regard, but still has all of the downsides that come with acquiring a write lock on the
/// atom table.
#[inline]
pub(super) unsafe fn get_data_or_insert_static(
    name: &'static str,
) -> Result<NonNull<AtomData>, AtomError> {
    with_atom_table(|mut atoms| atoms.get_data_or_insert_static(name))
}

/// Gets the atom with the given name from the global atom table, or inserts it as a new atom if not
/// present.
///
/// This operation acquires a write lock on the atom table, which requires exclusive access. This
/// means this operation must wait until all pre-existing readers/writers have released their locks,
/// and will block all subsequent readers/writers until it has completed. Since this is relatively
/// expensive, this should only be used when creating new atoms.
#[inline]
pub(super) unsafe fn get_data_or_insert(name: &str) -> Result<NonNull<AtomData>, AtomError> {
    with_atom_table(|mut atoms| atoms.get_data_or_insert(name))
}

/// Checks the global atom table for an atom by the given name, or returns None.
///
/// This operation acquires a read lock on the atom table, and so can run concurrently with other
/// readers, but will block if a writer is currently inserting into the table.
#[inline]
pub(super) fn get_data(name: &str) -> Option<NonNull<AtomData>> {
    with_atom_table_readonly(|atoms| atoms.get_data(name))
}

#[inline]
pub fn with_atom_table_readonly<F, T>(callback: F) -> T
where
    F: FnOnce(RwLockReadGuard<'static, AtomTable>) -> T,
{
    let atoms = ATOMS.get_or_init(|| RwLock::new(AtomTable::default()));
    callback(atoms.read())
}

#[inline]
pub fn with_atom_table<F, T>(callback: F) -> T
where
    F: FnOnce(RwLockWriteGuard<'static, AtomTable>) -> T,
{
    let atoms = ATOMS.get_or_init(|| RwLock::new(AtomTable::default()));
    callback(atoms.write())
}

#[derive(Default)]
pub struct GlobalAtomTable;
impl firefly_bytecode::AtomTable for GlobalAtomTable {
    type Atom = Atom;
    type AtomError = AtomError;
    type Guard = RwLockWriteGuard<'static, AtomTable>;

    fn len(&self) -> usize {
        with_atom_table_readonly(|atoms| atoms.len())
    }

    fn iter<'a>(&'a self) -> impl Iterator<Item = Self::Atom> + 'a {
        core::iter::empty()
    }

    fn get_or_insert(&mut self, s: &str) -> Result<Self::Atom, Self::AtomError> {
        match s {
            "false" => Ok(super::atoms::False),
            "true" => Ok(super::atoms::True),
            other => Ok(Atom(with_atom_table(|mut atoms| {
                atoms.get_data_or_insert(other)
            })?)),
        }
    }

    fn change<F, T>(&mut self, callback: F) -> T
    where
        F: FnOnce(&mut Self::Guard) -> T,
    {
        with_atom_table(|mut atoms| callback(&mut atoms))
    }
}

/// This struct represents the atom table, of which a program will only ever have one at a time,
/// with static lifetime. The atoms it contains are never collected.
pub struct AtomTable {
    ids: BTreeMap<&'static str, NonNull<AtomData>>,
    arena: DroplessArena,
}
impl firefly_bytecode::AtomTable for AtomTable {
    type Atom = Atom;
    type AtomError = AtomError;
    type Guard = Self;

    fn len(&self) -> usize {
        self.ids.len()
    }

    fn iter<'a>(&'a self) -> impl Iterator<Item = Self::Atom> + 'a {
        self.ids.values().map(|ptr| Atom(*ptr))
    }

    fn get_or_insert(&mut self, s: &str) -> Result<Self::Atom, Self::AtomError> {
        match s {
            "false" => Ok(super::atoms::False),
            "true" => Ok(super::atoms::True),
            other => Ok(Atom(self.get_data_or_insert(other)?)),
        }
    }

    fn change<F, T>(&mut self, callback: F) -> T
    where
        F: FnOnce(&mut Self::Guard) -> T,
    {
        callback(self)
    }
}

// By default, `NonNull<T>` is neither send nor sync, as such pointers may alias, however, in our
// case, the pointers are to data which is pinned, 'static, read-only, and does not support interior
// mutability, making such pointers trivially Send and Sync.
//
// Furthermore, to mutate the atom table (by adding new atoms), one has to acquire an exclusive
// write lock, which guarantees that the table itself is also Send and Sync
unsafe impl Send for AtomTable {}
unsafe impl Sync for AtomTable {}
impl Default for AtomTable {
    fn default() -> Self {
        Self {
            ids: BTreeMap::new(),
            arena: DroplessArena::default(),
        }
    }
}
impl fmt::Debug for AtomTable {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        for (_name, data) in self.ids.iter() {
            let atom = Atom(*data);
            writeln!(f, "atom(value = {}, ptr = {:p})", &atom, data)?;
        }
        Ok(())
    }
}
impl AtomTable {
    pub fn len(&self) -> usize {
        self.ids.len()
    }

    pub fn iter(&self) -> impl Iterator<Item = NonNull<AtomData>> + '_ {
        self.ids.values().copied()
    }

    fn extend(&mut self, data: &'static [AtomData]) {
        for atom in data {
            let ptr = unsafe { NonNull::new_unchecked(atom as *const AtomData as *mut AtomData) };
            let name = unsafe { atom.as_str().unwrap() };
            self.ids.entry(name).or_insert(ptr);
        }
    }

    fn get_data(&self, name: &str) -> Option<NonNull<AtomData>> {
        self.ids.get(name).copied()
    }

    pub fn get_data_or_insert(&mut self, name: &str) -> Result<NonNull<AtomData>, AtomError> {
        match self.get_data(name) {
            Some(existing_id) => Ok(existing_id),
            None => unsafe { self.insert(name) },
        }
    }

    // SAFETY: See insert_static for the safety constraints
    pub unsafe fn get_data_or_insert_static(
        &mut self,
        name: &'static str,
    ) -> Result<NonNull<AtomData>, AtomError> {
        match self.get_data(name) {
            Some(existing_id) => Ok(existing_id),
            None => self.insert_static(name),
        }
    }

    // SAFETY: You must ensure two things about the string passed to this function:
    //
    // * The string reference MUST have been derived from a CStr pointer. This is because
    // we assume that the pointer the reference is based on can be safely treated as a pointer
    // to a null-terminated string. If it can't, bad things will happen.
    // * The string you pass to this function MUST be truly valid for static lifetime, or
    // the behavior is undefined.
    //
    // This is intended to avoid wasting space on atoms which are already
    // stored in the read-only atom section constructed by the linker. This data is always valid for
    // the static lifetime, and so we can construct `&'static str` from them safely.
    unsafe fn insert_static(&mut self, name: &'static str) -> Result<NonNull<AtomData>, AtomError> {
        let bytes = name.as_bytes();
        let data = self.alloc_data(AtomData {
            ptr: bytes.as_ptr(),
            size: bytes.len(),
        });
        self.ids.insert(name, data);

        Ok(data)
    }

    // This function is used to insert new atoms in the table during runtime
    // SAFETY: `name` must have been checked as not existing while holding the current mutable
    // reference.
    unsafe fn insert(&mut self, name: &str) -> Result<NonNull<AtomData>, AtomError> {
        use core::intrinsics::unlikely;

        if unlikely(name.len() == 0) {
            let data = self.alloc_data(AtomData {
                ptr: ptr::null_mut(),
                size: 0,
            });
            self.ids.insert("", data);

            return Ok(data);
        }

        // Allocate memory for atom metadata and value
        let bytes = name.as_bytes();
        let size = bytes.len();
        let (layout, value_offset) = Layout::new::<AtomData>()
            .extend(Layout::from_size_align_unchecked(
                size,
                mem::align_of::<u8>(),
            ))
            .unwrap();
        let layout = layout.pad_to_align();
        let ptr = self.arena.alloc_raw(layout);

        let value_ptr = ptr.add(value_offset);
        let data_ptr: *mut AtomData = ptr.cast();
        // Write metadata
        data_ptr.write(AtomData {
            ptr: value_ptr,
            size,
        });
        // Write atom data
        ptr::copy_nonoverlapping(bytes.as_ptr(), value_ptr, size);

        let data = NonNull::new_unchecked(data_ptr);

        // Register in atom table
        self.ids.insert(data.as_ref().as_str().unwrap(), data);

        Ok(data)
    }

    unsafe fn alloc_data(&mut self, data: AtomData) -> NonNull<AtomData> {
        let layout = Layout::new::<AtomData>();

        let ptr = self.arena.alloc_raw(layout) as *mut AtomData;
        ptr.write(data);

        NonNull::new_unchecked(ptr)
    }
}
